{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyramidStereoMatchingNetworkImplementation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TISAh5pdVL01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IUtsP0mHcmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip KITTI_Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ipb0JqYFL6Mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "from keras import layers, models, utils\n",
        "import keras\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM6bBMLXQDHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Hi,Wi= int(376/2),int(1240/2)\n",
        "H,W,C = 128,512,3\n",
        "batch_size=4\n",
        "images = 200\n",
        "training_count = 180"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlkR38NFMjbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.zeros((images,H,W,C,2), dtype=np.float32)\n",
        "disparity = np.zeros((images,H,W), dtype=np.float32)\n",
        "\n",
        "for i in range(200):\n",
        "  if i < 10:\n",
        "    train[i,:,:,:,0] = np.array(Image.open(f\"KITTI_Data/Left_Images/00000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    train[i,:,:,:,1] = np.array(Image.open(f\"KITTI_Data/Right_Images/00000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    disparity[i,:,:] = np.array(Image.open(f\"KITTI_Data/DisparityMap/00000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "  elif i < 100:\n",
        "    train[i,:,:,:,0] = np.array(Image.open(f\"KITTI_Data/Left_Images/0000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    train[i,:,:,:,1] = np.array(Image.open(f\"KITTI_Data/Right_Images/0000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    disparity[i,:,:] = np.array(Image.open(f\"KITTI_Data/DisparityMap/0000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "  else:\n",
        "    train[i,:,:,:,0] = np.array(Image.open(f\"KITTI_Data/Left_Images/000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    train[i,:,:,:,1] = np.array(Image.open(f\"KITTI_Data/Right_Images/000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "    disparity[i,:,:] = np.array(Image.open(f\"KITTI_Data/DisparityMap/000{i}_10.png\").resize((Wi,Hi)))[-H:,:W]\n",
        "disparity = disparity/200.0\n",
        "train = train/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhxhyNhAiWb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiplyDimension(layers.Layer):\n",
        "\n",
        "  def call(self, x):\n",
        "    pos = np.arange(int(x.shape[1]), dtype=np.float32)\n",
        "    arr = np.zeros((int(x.shape[0]),int(x.shape[1]),int(x.shape[2]),int(x.shape[3]),int(x.shape[4])),dtype=np.float32)+pos[np.newaxis,:,np.newaxis,np.newaxis,np.newaxis]\n",
        "    pos = tf.convert_to_tensor(arr)\n",
        "    x *= pos\n",
        "    return x\n",
        "\n",
        "class ShiftRight(layers.Layer):\n",
        "  def __init__(self, shift_count, left = True, **kwargs):\n",
        "    self.shift_count = shift_count\n",
        "    self.left = left\n",
        "    super(ShiftRight, self).__init__(**kwargs)\n",
        "\n",
        "\n",
        "  def call(self, right):\n",
        "    if (self.left == True): ##Remove first few rows of left\n",
        "      x = tf.concat([tf.zeros([tf.shape(right)[0],right.shape[1],self.shift_count,right.shape[3]]),right[:,:,self.shift_count:]],axis=2)\n",
        "    else: ##Remove last few rows of right\n",
        "      x = tf.concat([tf.zeros([tf.shape(right)[0],right.shape[1],self.shift_count,right.shape[3]]),right[:,:,:-self.shift_count]],axis=2)\n",
        "    return x\n",
        "\n",
        "def conv2(x,filter_count,kernel_size=(3,3),stride=1,dilation=(1,1),padding='same',use_bias=True,alpha=0.1,ifrelu=True,ifNorm=True,ifUpsample=False,output_padding=[0,0]):\n",
        "  if (ifUpsample == True):\n",
        "    x = layers.Conv2DTranspose(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),padding=padding,use_bias=use_bias,output_padding=output_padding)(x) \n",
        "  else:\n",
        "    x = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)(x)\n",
        "  if (ifNorm == True):\n",
        "    x = layers.BatchNormalization()(x)\n",
        "  if (ifrelu == True):\n",
        "    layers.LeakyReLU(alpha=alpha)(x)\n",
        "  return x\n",
        "\n",
        "\n",
        "def conv3(x,filter_count,kernel_size=(3,3,3),stride=1,padding='same',use_bias=True,alpha=0.1,ifrelu=True,ifNorm=True,ifUpsample=False,output_padding=[0,0,0]):\n",
        "  if (ifUpsample == True):\n",
        "    x = layers.Conv3DTranspose(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride,stride),padding=padding,use_bias=use_bias,output_padding=output_padding)(x) \n",
        "  else:\n",
        "    x = layers.Conv3D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride,stride),padding=padding,use_bias=use_bias)(x)\n",
        "  if (ifNorm == True):\n",
        "    x = layers.BatchNormalization()(x)\n",
        "  if (ifrelu == True):\n",
        "    layers.LeakyReLU(alpha=alpha)(x)\n",
        "  return x\n",
        "\n",
        "def afterConv(x):\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  return x\n",
        "\n",
        "def makeBranch(x,branchx,pool_size):\n",
        "  branch = layers.AveragePooling2D(pool_size=(pool_size,pool_size))(x)\n",
        "  branch = branchx(branch)\n",
        "  branch = afterConv(branch)\n",
        "  branch = layers.Lambda( \n",
        "            lambda image: tf.image.resize_images( \n",
        "              image, (H//4, W//4), \n",
        "              method = tf.image.ResizeMethod.BILINEAR,\n",
        "              align_corners = True, preserve_aspect_ratio = False\n",
        "            )\n",
        "          )(branch)\n",
        "  return branch\n",
        "\n",
        "def SPP(x,y,branch1,branch2,branch3,branch4,fusion1,fusion2, filter_count = int(16/4)):\n",
        "  branch1 = makeBranch(x,branch1,int(filter_count*8))\n",
        "  branch2 = makeBranch(x,branch2,int(filter_count*4))\n",
        "  branch3 = makeBranch(x,branch3,int(filter_count*2))\n",
        "  branch4 = makeBranch(x,branch4,filter_count)\n",
        "  \n",
        "  fusion = layers.Concatenate()([x,y,branch1,branch2,branch3,branch4])\n",
        "  fusion = fusion1(fusion)\n",
        "  fusion = afterConv(fusion)\n",
        "  fusion = fusion2(fusion)\n",
        "  fusion = afterConv(fusion)\n",
        "  return fusion\n",
        "\n",
        "def cost_volume(x,y):\n",
        "  return layers.Concatenate()([x,y])\n",
        "\n",
        "\n",
        "def ResLayer(x,filter_count):\n",
        "  y = conv3(x,filter_count)\n",
        "  y = conv3(y,filter_count)\n",
        "  y = layers.add([y,x])\n",
        "  return y\n",
        "\n",
        "\n",
        "def CNN3D_Basic(x,skipcount,filter_count=16):\n",
        "  x = conv3(x,filter_count)\n",
        "  x = conv3(x,filter_count)\n",
        "  for i in range(4):\n",
        "    x = ResLayer(x,filter_count)\n",
        "\n",
        "  x = conv3(x,filter_count)\n",
        "  x = conv3(x,1)\n",
        "  x = layers.UpSampling3D(size=(skipcount,4,4))(x)\n",
        "  x = layers.Softmax(axis=1)(x)\n",
        "  x = MultiplyDimension()(x)\n",
        "  x = layers.Lambda(lambda xin: K.sum(xin, axis=1))(x)\n",
        "  return x\n",
        "\n",
        "def CNN3D(x,skipcount=4,base_filter_count=16):\n",
        "  x = conv3(x,base_filter_count)\n",
        "  x = conv3(x,base_filter_count)\n",
        "  y = conv3(x,base_filter_count)\n",
        "  y = conv3(y,base_filter_count)\n",
        "\n",
        "  x = layers.add([x,y])\n",
        "  y = conv3(x,base_filter_count,stride=2) ## 1/8\n",
        "  y = conv3(y,base_filter_count)\n",
        "\n",
        "  z = conv3(y,base_filter_count,stride=2) ## 1/16\n",
        "  z = conv3(z,base_filter_count)\n",
        "  z = conv3(z,base_filter_count,stride=2,ifUpsample=True) ## 1/8\n",
        "  z = layers.add([z,y])\n",
        "\n",
        "  a = conv3(z,base_filter_count,stride=2,ifUpsample=True,output_padding=[1,1,1]) ##1/4\n",
        "  a = layers.add([a,x])\n",
        "\n",
        "  aside_1 = conv3(a,base_filter_count)\n",
        "  aside_1 = conv3(aside_1,base_filter_count)\n",
        "  output1 = layers.UpSampling3D(size=(skipcount,4,4))(aside_1)\n",
        "  output1 = layers.Softmax(axis=1)(output1)\n",
        "  output1 = MultiplyDimension()(output1)\n",
        "  output1 = layers.Lambda(lambda xin: K.sum(xin, axis=1))(output1)\n",
        "\n",
        "  a = conv3(a,base_filter_count,stride=2) ## 1/8\n",
        "  a = conv3(a,base_filter_count)\n",
        "  z = layers.add([a,z])\n",
        "\n",
        "  z = conv3(z,base_filter_count,stride=2) ## 1/16\n",
        "  z = conv3(z,base_filter_count)\n",
        "  z = conv3(z,base_filter_count,stride=2,ifUpsample=True) ## 1/8\n",
        "  z = layers.add([z,y])\n",
        "\n",
        "  a = conv3(z,base_filter_count,stride=2,ifUpsample=True,output_padding=[1,1,1]) ## 1/4\n",
        "  a = layers.add([a,x])\n",
        "\n",
        "  aside_2 = conv3(a,base_filter_count)\n",
        "  aside_2 = conv3(aside_2,base_filter_count)\n",
        "  aside_2 = layers.add([aside_2,aside_1])\n",
        "  output2 = layers.UpSampling3D(size=(skipcount,4,4))(aside_2)\n",
        "  output2 = layers.Softmax(axis=1)(output2)\n",
        "  output2 = MultiplyDimension()(output2)\n",
        "  output2 = layers.Lambda(lambda xin: K.sum(xin, axis=1))(output2)\n",
        "\n",
        "  a = conv3(a,base_filter_count,stride=2) ## 1/8\n",
        "  a = conv3(a,base_filter_count)\n",
        "  z = layers.add([a,z])\n",
        "  z = conv3(z,base_filter_count,stride=2) ## 1/16\n",
        "  z = conv3(z,base_filter_count)\n",
        "  z = conv3(z,base_filter_count,stride=2,ifUpsample=True) ## 1/8\n",
        "  y = layers.add([z,y])\n",
        "\n",
        "  y = conv3(y,base_filter_count,stride=2,ifUpsample=True,output_padding=[1,1,1]) ## 1/4\n",
        "  x = layers.add([y,x])\n",
        "\n",
        "  aside_3 = conv3(x,base_filter_count)\n",
        "  aside_3 = conv3(aside_3,base_filter_count)\n",
        "  aside_3 = layers.add([aside_3,aside_2])\n",
        "  output3 = layers.UpSampling3D(size=(skipcount,4,4))(aside_3)\n",
        "  output3 = layers.Softmax(axis=1)(output3)\n",
        "  output3 = MultiplyDimension()(output3)\n",
        "  output3 = layers.Lambda(lambda xin: K.sum(xin, axis=1))(output3)\n",
        "\n",
        "  return [output1,output2,output3]\n",
        "\n",
        "def sharedCNN(x, layers = [], arraysOfLayers = []):\n",
        "  conv2_16 = x\n",
        "  for i in layers:\n",
        "    x = i(x)\n",
        "    x = afterConv(x)\n",
        "  for i in arraysOfLayers:\n",
        "    for j in range(len(i)):\n",
        "      x = i[j](x)\n",
        "      x = afterConv(x)\n",
        "      if j == 15:\n",
        "        conv2_16 = x\n",
        "  conv4_3 = x\n",
        "  return [conv2_16,conv4_3]\n",
        "\n",
        "\n",
        "def PSMN(left_input,right_input,disparity = -1,skipcount=4, base_filter_count = 16,basic3DCNN = True):\n",
        "  if disparity==-1:\n",
        "    disparity = right_input.shape[2]//4\n",
        "  filter_count = base_filter_count; kernel_size = 3; dilation = (1,1); padding = 'same'; use_bias = True; stride = 1\n",
        "\n",
        "  ##CNN Layers\n",
        "  conv0_1 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(2,2),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  conv0_2 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  conv0_3 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  conv1x = []\n",
        "  conv2x = []\n",
        "  conv3x = []\n",
        "  conv4x = []\n",
        "\n",
        "  for i in range(3):\n",
        "    conv1x.append(layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias))\n",
        "\n",
        "  filter_count = int(base_filter_count*2)\n",
        "  conv2x.append(layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(2,2),dilation_rate=dilation,padding=padding,use_bias=use_bias))\n",
        "  for i in range(1,16):\n",
        "    conv2x.append(layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias))\n",
        "\n",
        "  filter_count = int(base_filter_count*4)\n",
        "  dilation = (2,2)\n",
        "  for i in range(3):\n",
        "    conv3x.append(layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias))\n",
        "  \n",
        "  dilation = (4,4)\n",
        "  for i in range(3):\n",
        "    conv4x.append(layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias))\n",
        "\n",
        "  ###SPP LAYERS\n",
        "  filter_count = base_filter_count\n",
        "  branch_1 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  branch_2 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  branch_3 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  branch_4 = layers.Conv2D(filters=filter_count,kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  fusion1 = layers.Conv2D(filters=int(base_filter_count*4),kernel_size=kernel_size,strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "  fusion2 = layers.Conv2D(filters=base_filter_count,kernel_size=(1,1),strides=(stride,stride),dilation_rate=dilation,padding=padding,use_bias=use_bias)\n",
        "\n",
        "\n",
        "  ##Make Cost Volume\n",
        "  costvolume_arr = []\n",
        "  left = left_input\n",
        "  right = right_input\n",
        "  for i in range(disparity//skipcount): \n",
        "    if i != 0:\n",
        "      right = ShiftRight(skipcount,left=False)(right); left = ShiftRight(skipcount, left=True)(left)\n",
        "    \n",
        "    rconv4_3,rconv2_16 = sharedCNN(right,[conv0_1,conv0_2,conv0_3],[conv1x,conv2x,conv3x,conv4x])\n",
        "    r_spp = SPP(rconv4_3,rconv2_16,branch_1,branch_2,branch_3,branch_4,fusion1,fusion2,filter_count=int(base_filter_count/4))\n",
        "    lconv4_3,lconv2_16 = sharedCNN(left,[conv0_1,conv0_2,conv0_3],[conv1x,conv2x,conv3x,conv4x])\n",
        "    l_spp = SPP(lconv4_3,lconv2_16,branch_1,branch_2,branch_3,branch_4,fusion1,fusion2,filter_count=int(base_filter_count/4))\n",
        "\n",
        "    costvolume_i = cost_volume(l_spp,r_spp)\n",
        "    costvolume_i = layers.Reshape((int(1),int(costvolume_i.shape[1]),int(costvolume_i.shape[2]),int(costvolume_i.shape[3])))(costvolume_i)\n",
        "    costvolume_arr.append(costvolume_i)\n",
        "  \n",
        "  costvolume = layers.Concatenate(axis=1)(costvolume_arr)\n",
        "\n",
        "  ##Model Output and Make Model\n",
        "  if basic3DCNN == True:\n",
        "    output = CNN3D_Basic(costvolume,skipcount,filter_count=base_filter_count)\n",
        "    return Model(inputs=[left_input,right_input],outputs=[output])\n",
        "  else:\n",
        "    output1,output2,output3 = CNN3D(costvolume,skipcount,filter_count=base_filter_count)\n",
        "    return Model(inputs=[left_input,right_input],outputs=[output1,output2,output3])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BujnUR-UsKFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "left_input = layers.Input(batch_shape=(batch_size,H,W,C))\n",
        "right_input = layers.Input(batch_shape=(batch_size,H,W,C))\n",
        "\n",
        "psmn = PSMN(left_input,right_input,disparity=192,skipcount=4,base_filter_count=32,basic3DCNN=False)\n",
        "psmn.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1zdNQ430FO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class SaveWeights(tf.keras.callbacks.Callback):\n",
        "#   def on_epoch_end(self, epoch, logs=None):\n",
        "    #keras.models.save_model(psmn,'psmn.h5')\n",
        "    #files.download('psmn.h5')\n",
        "def smoothL1(y_true, y_pred, HUBER_DELTA = 1.0):\n",
        "   x   = K.abs(y_true - y_pred)\n",
        "   x   = K.switch(x < HUBER_DELTA, 0.5 * x ** 2, HUBER_DELTA * (x - 0.5 * HUBER_DELTA))\n",
        "   return  K.sum(x)\n",
        "psmn.compile(optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=True),loss=smoothL1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np4_VueYDlGB",
        "colab_type": "code",
        "outputId": "0e1e4feb-77dc-4f8f-ae59-312f56fa6174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "###10 MINUTES PER EPOCH ON LOCAL MACHINE\n",
        "psmn.fit(x=[train[:training_count,:,:,:,0],train[:training_count,:,:,:,1]],y=[disparity[:training_count,:,:,np.newaxis]],\n",
        "         batch_size=batch_size,epochs=100,validation_data=([train[training_count:,:,:,:,0],train[training_count:,:,:,:,1]],\n",
        "        [disparity[training_count:,:,:,np.newaxis]]),callbacks=[keras.callbacks.ModelCheckpoint('psmn_weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)]\n",
        "         )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 180 samples, validate on 20 samples\n",
            "Epoch 1/100\n",
            "180/180 [==============================] - 2210s 12s/step - loss: 1679806.6618 - val_loss: 1235524.7937\n",
            "Epoch 2/100\n",
            "180/180 [==============================] - 192s 1s/step - loss: 1128159.7264 - val_loss: 1169103.4438\n",
            "Epoch 3/100\n",
            "180/180 [==============================] - 193s 1s/step - loss: 1114738.3778 - val_loss: 1276468.7812\n",
            "Epoch 4/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 1109292.4764 - val_loss: 1230715.2563\n",
            "Epoch 5/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 1098530.7021 - val_loss: 1312556.4812\n",
            "Epoch 6/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 1085534.0222 - val_loss: 1710335.4375\n",
            "Epoch 7/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 1070207.6222 - val_loss: 1448171.5750\n",
            "Epoch 8/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 1065920.2986 - val_loss: 1708442.5000\n",
            "Epoch 9/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 1037643.5542 - val_loss: 1383637.1500\n",
            "Epoch 10/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 1056610.5417 - val_loss: 1451170.6187\n",
            "Epoch 11/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 1030103.3299 - val_loss: 1089108.0188\n",
            "Epoch 12/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 1013027.1437 - val_loss: 1180728.7063\n",
            "Epoch 13/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 1001794.9528 - val_loss: 1143058.0938\n",
            "Epoch 14/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 998245.3597 - val_loss: 1152907.8687\n",
            "Epoch 15/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 995498.6354 - val_loss: 1313587.7688\n",
            "Epoch 16/100\n",
            "180/180 [==============================] - 193s 1s/step - loss: 988282.1486 - val_loss: 1271235.5625\n",
            "Epoch 17/100\n",
            "180/180 [==============================] - 194s 1s/step - loss: 972206.4799 - val_loss: 1063350.2563\n",
            "Epoch 18/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 980254.5528 - val_loss: 1141443.2500\n",
            "Epoch 19/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 963660.0639 - val_loss: 1196537.6625\n",
            "Epoch 20/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 942207.4757 - val_loss: 1119777.0000\n",
            "Epoch 21/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 948691.5403 - val_loss: 1245333.0125\n",
            "Epoch 22/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 934804.9507 - val_loss: 1038586.5312\n",
            "Epoch 23/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 924247.3438 - val_loss: 1043625.0062\n",
            "Epoch 24/100\n",
            "180/180 [==============================] - 198s 1s/step - loss: 918185.1937 - val_loss: 1019466.2500\n",
            "Epoch 25/100\n",
            "180/180 [==============================] - 196s 1s/step - loss: 910348.8715 - val_loss: 1031454.3250\n",
            "Epoch 26/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 910608.6021 - val_loss: 1071946.6687\n",
            "Epoch 27/100\n",
            "180/180 [==============================] - 197s 1s/step - loss: 908339.2153 - val_loss: 1084713.3687\n",
            "Epoch 28/100\n",
            "180/180 [==============================] - 198s 1s/step - loss: 895942.0687 - val_loss: 1019422.0875\n",
            "Epoch 29/100\n",
            "180/180 [==============================] - 197s 1s/step - loss: 914821.4757 - val_loss: 1062345.8313\n",
            "Epoch 30/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 895480.9618 - val_loss: 1058557.3875\n",
            "Epoch 31/100\n",
            "180/180 [==============================] - 195s 1s/step - loss: 888535.4188 - val_loss: 992485.2562\n",
            "Epoch 32/100\n",
            " 92/180 [==============>...............] - ETA: 1:33 - loss: 883615.1481"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxcqc8HsebIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = np.zeros((12,H,W,1))\n",
        "test[0:4] = psmn.predict(x=[train[training_count-18:training_count-6:3,:,:,:,0],train[training_count-6:training_count-2,:,:,:,1]])\n",
        "test[4:8] = psmn.predict(x=[train[training_count-6:training_count+6:3,:,:,:,0],train[training_count-2:training_count+2,:,:,:,1]])\n",
        "test[8:12] = psmn.predict(x=[train[training_count+6:training_count+18:3,:,:,:,0],train[training_count+2:training_count+6,:,:,:,1]])\n",
        "\n",
        "real = disparity[162:198:3]*100\n",
        "test = test*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8B_mY8HebbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f, ax = plt.subplots(6,4, gridspec_kw={'wspace':0.001,'hspace':0.35},figsize=(32,8))\n",
        "print(\"Current Number of Epochs: 150; Left Pairs from Training Set. Right Pairs From Validation Set.\")\n",
        "num = np.where(np.abs(real-test[:,:,:,0])<=300,1,0)\n",
        "correct = np.sum(num)/(H*W*12)\n",
        "print(f\"Percentage Correct (Model Output Within 3px of Actual Disparity) = {correct}\")\n",
        "num = np.where(np.abs(real-test[:,:,:,0])<=800,1,0)\n",
        "correct = np.sum(num)/(H*W*12)\n",
        "print(f\"Percentage Correct (Model Output Within 8px of Actual Disparity) = {correct}\")\n",
        "for i in range(4):\n",
        "    plt.gray()\n",
        "    type_ = 'Training'\n",
        "    ax[0,i].imshow(test[i,:,:,0]);  ax[0,i].axis('Off'); ax[0,i].set_title(f'Output {type_}', size=16)\n",
        "    ax[1,i].imshow(real[i]);  ax[1,i].axis('Off'); ax[1,i].set_title('Truth', size=16)\n",
        "    if i >1:\n",
        "      type_ = 'Validation'\n",
        "    ax[2,i].imshow(test[i+4,:,:,0]);  ax[2,i].axis('Off'); ax[2,i].set_title(f'Output {type_}', size=16)\n",
        "    ax[3,i].imshow(real[i+4]);  ax[3,i].axis('Off'); ax[3,i].set_title('Truth', size=16)\n",
        "    type_ = 'Validation'\n",
        "    ax[4,i].imshow(test[i+8,:,:,0]);  ax[4,i].axis('Off'); ax[4,i].set_title(f'Output {type_}', size=16)\n",
        "    ax[5,i].imshow(real[i+8]);  ax[5,i].axis('Off'); ax[5,i].set_title('Truth', size=16)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}